	
	### Função de Ativação

	As funções de ativação são um elemento extremamente importante das redes neurais artificiais. Elas basicamente decidem se um neurônio deve ser ativado ou não. Ou seja, se a informação que o neurônio está recebendo é relevante para a informação fornecida ou deve ser ignorada. Veja na fórmula abaixo como a função de ativação é mais uma camada matemática no processamento.

	imagem{funcao_ativacao.png}

	A função de ativação é a transformação não linear que fazemos ao longo do sinal de entrada. Esta saída transformada é então enviada para a próxima camada de neurônios como entrada. Quando não temos a função de ativação, os pesos e bias simplesmente fazem uma transformação linear. Uma equação linear é simples de resolver, mas é limitada na sua capacidade de resolver problemas complexos. Uma rede neural sem função de ativação é essencialmente apenas um modelo de regressão linear. A função de ativação faz a transformação não-linear nos dados de entrada, tornando-o capaz de aprender e executar tarefas mais complexas. Queremos que nossas redes neurais funcionem em tarefas complicadas, como traduções de idiomas (Processamento de Linguagem Natural) e classificações de imagens (Visão Computacional). As transformações lineares nunca seriam capazes de executar tais tarefas[2].
	Tipos mais populares de função de ativação: Função Linear, binaria, Sigmóide, ReLU, Softmax entre outras.
	A Escolha da funçãode ativação vai depender do problema.

	Funções Sigmóide e suas combinações geralmente funcionam melhor no caso de classificadores.
    	A função ReLU é uma função de ativação geral e é usada na maioria dos casos atualmente.
    	Se encontrarmos um caso de neurônios deficientes em nossas redes, a função Leaky ReLU é a melhor escolha.
    	Tenha sempre em mente que a função ReLU deve ser usada apenas nas camadas ocultas.
    	Como regra geral, você pode começar usando a função ReLU e depois passar para outras funções de ativação no caso da ReLU não fornecer resultados ótimos[2].

	### Backpropagation	

	### Problemas no Processamento de Imagens por Redes Clássicas	

	### Deep Learning

	### Redes Neuronais Convolucionais
	
	No contexto de inteligencia artificial e aprendizagem de maquina, uma rede neural convolucional (CNN do inglês Convolutional Neural network ou ConvNet) é uma classe de rede neural artificial do tipo feed-foward, que vem sendo aplicada com sucesso no processamento e analise de imagens digitais[4].
	Uma CNN tende a demandar um nível mínimo de pré-processamento quando comparada a outros algoritmos de classificação de imagens. Isso significa que a rede "aprende" os filtros que em um algoritmo tradicional precisariam ser implementados manualmente. Essa independência de um conhecimento a priori e do esforço humano no desenvolvimento de suas funcionalidades básicas pode ser considerada a maior vantagem de sua aplicação.
	Esse tipo de rede é usada principalmente em reconhecimento de imagens e processamento de vídeo, embora já tenha sido aplicada com sucesso em experimentos envolvendo processamento de voz e linguagem natural.
	Uma CNN é normalmente composta por camadas que geralmente executam uma série de quatro operações lineares e não-lineares sequenciais, tais como:
    • Convolução: calcula a intensidade de um dado pixel de acordo com a soma ponderada dos seus pixeis vizinhos.
    • Ativação: mapeia a saída da operação de convolução com o objetivo de limitar o ”neurônio“ de saída.
    • Spatial Pooling: reduz a dimensão das características do mapa. Tem o efeito de reduzir a sensibilidade da saída do mapa a deslocamentos e outras formas de distorção, selecionando características invariantes que melhoram o desempenho de generalização.
    • Normalização: geralmente cria uma concorrência entre os mapas. Visa reforçar ainda mais a robustez da CNN quanto ao vetor de características de saída.


[2] GONÇALVES DOS SANTOS, Claudio Filipi; BREVE, Fabricio Aparecido. Optical Character Recognition Usando Deep Learning In: VII Workshop do Programa de Pós- Graduação em Ciência da Computação - UNESP, 2017, Rio Claro. VII Workshop do Programa de Pós- Graduação em Ciência da Computação - UNESP, 2017. p.15 - 16.
[3] Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85–117, 2015.
[4] Convolutional Neural Networks (LeNet) - DeepLearning 0.1 documentation. DeepLearning 0.1. LISA Lab. Acesso em: dezembro de 2017.
[5] Deep Learning Book, disponível on line em Livro http://www.deeplearningbook.com.br. Data Science Academy. 2018.
[8] GOODFELLOW, I.; BENGIO, Y.; COURVILLE, A. Deep Learning. [S.l.]: MIT Press, 2018. http://www.deeplearningbook.org.



 
