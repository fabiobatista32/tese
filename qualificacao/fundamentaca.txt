FUNDAMENTAÇÃO TEORICA

	Introdução à Redes Neuronais

	As redes Neuronais Artificiais tem sua história iniciada quando um ramo da psicologia caracterizado como conexionismo, tenta descrever matematicamente a forma de aprendizado do cérebro animal, tendo como referência a estrutura básica da massa encefálica, o neurônio. O primeiro modelo artificial de um neurônio natural foi desenvolvido por Warren McCulloch e Walter Pitts em 1943.
	Em 1949 Donald Hebb mostrou que a capacidade de aprendizado da rede pode ser aprimorada pelo ajuste dos pesos de entrada dos neurônios, propondo que o aprendizado dos neurônios poderia ser realizado pelo reforço das ligações sinápticas entre os neurônios excitados pela camada de entrada. Em 1958 Rosenblatt propôs uma rede (Perceptron) com base na estrutura do neurônios proposto por McCulloch e Pitts onde, através do ajuste dos pesos sinápticos ocorreria o aprendizado de certos tipos de padrões.
	Nos trabalhos de Widrow e Hoff em 1960 foi proposta uma regra de aprendizado cujos os valores dos pesos seriam proporcionais a sua entrada, o erro entre o valor desejado na saı́da da rede e o valor apresentado pela mesma e uma constante (caracterizada como
passo de aprendizado). As primeiras redes sofreram problemas quanto a classificação de padrões não-linearmente-separáveis, resolvido através do uso de redes MLP (Multi-Layer Perceptron) ou Perceptron de múltiplas camadas e do algoritmo de backpropagation.
	As Redes Neuronais Artificiais tem se mostrado robustas à várias aplicações práticas como cancelamento de eco, reconhecimento de padrões, indo até extração de padrões de uma grande quantidade de imagens [1] [15] [16] [8]. A rede neuronal, pela sua constituição matemática, pode ser vista como um dispositivo de computação paralela, haja vista que um neurônio trata vários sinais paralelamente, gerando um efeito individual entradas-saı́da [16].
	
		

	Função de Ativação

	Backpropagation	

	Problemas no Processamento de Imagens por Redes Clássicas	

	Deep Learning

	O interesse pela Aprendizagem de Máquina( Machine learning) explodiu na última década. O mundo a nossa volta está passando por uma transformação e vemos uma interação cada vez maior das aplicações de computador com os seres humanos.

	Redes Neuronais Convolucionais
	
		As camadas convolucionais são responsáveis por extrair atributos dos volumens de entradas.
		As camadas de pooling são responsáveis por reduzir a dimensionalidade do volume resultante após as camadas convolucionais e ajudam a tornar a representação invariante a pequenas translações na entrada.


[1] CHONG, E. K. P.; ZAK., S. H. An Introduction to Optimization, Fourth Edition. [S.l.]: Jon Wiley Sons, Inc, 2013.
[8] GOODFELLOW, I.; BENGIO, Y.; COURVILLE, A. Deep Learning. [S.l.]: MIT Press, 2018. http://www.deeplearningbook.org.
[15] SAXENA, A. 
[16] BRAGA, A. P. L. F. d. C. e. T. B. L. Antônio de P. Redes Neurais Artificiais-Teoria e Aplicações.-2ed. [S.l.]: LTC-Livros Técnicos e Cientı́ficos Editora S.A.-Rio de Janeiro, 2007.

Deep Learning Book, disponível on line em Livro http://www.deeplearningbook.com.br. Data Science Academy. 2018.
